{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1h4lkrMLAslM97zfGi17H5wV5IHwQM9xq",
      "authorship_tag": "ABX9TyMWQg4JvK47i2uTPxqQSqW8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nkrj01/Models-from-scratch/blob/main/NN_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "In this notebook, I've implemented a dense neural network from scratch for binary classification problem using the Autodiff algorithm. The algorithm is presented twice: the first version is hard-coded but offers a simpler understanding for someone learning how to implement backpropagation. The second version is more flexible, allowing users to easily adjust the number of hidden layers and nodes in each layer. This flexibility enhances customization while maintaining readability.\n",
        "\n",
        "The model was succesfully able to classify the breast cancer data from Sk-learn with ~97% accuracy"
      ],
      "metadata": {
        "id": "4Sfq2vN_uSv_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ohx0BlLL55l"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Helper Functions**"
      ],
      "metadata": {
        "id": "yvvIlSgbM7EN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigma(z):\n",
        "  # activation function\n",
        "  a = 1/(1 + np.exp(-z))\n",
        "  return a\n",
        "\n",
        "def make_binary(a):\n",
        "  # converting activation to binaries for final result\n",
        "  if a >= 0.5:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def binary_entropy_loss(y_true, y):\n",
        "  epsilon = 1e-15\n",
        "  y = np.clip(y, epsilon, 1 - epsilon)\n",
        "  loss = -(y_true*np.log(y) + (1-y_true)*np.log(1-y))\n",
        "  return loss\n",
        "\n",
        "def predict(x_test):\n",
        "  # prediction using forward propagation\n",
        "  a1 = x_test.reshape(-1, 1)\n",
        "  z2 = np.matmul(W1, a1) + b1\n",
        "  a2 = sigma(z2)\n",
        "  z3 = np.matmul(W2, a2) + b2\n",
        "  a3 = sigma(z3)\n",
        "  z4 = np.matmul(W3, a3) + b3\n",
        "  a4 = sigma(z4)\n",
        "  y = make_binary(a4)\n",
        "  return y\n",
        "\n",
        "def accuracy(x_test, y_test):\n",
        "  # Calculating binary accuracy of the model\n",
        "  y_predict = []\n",
        "  for row in x_test:\n",
        "    y_predict.append(predict(row))\n",
        "\n",
        "  y_predict = np.array(y_predict)\n",
        "  matching_elements = (y_predict == y_test)\n",
        "  count_matches = np.sum(matching_elements)\n",
        "  accuracy = count_matches/x_test.shape[0]\n",
        "  return accuracy\n",
        "\n",
        "def predict_2(x_test):\n",
        "  # prediction using forward propagation for 2nd version\n",
        "  a = []\n",
        "  z = []\n",
        "  a.append(x_test.reshape(-1, 1))\n",
        "  for i in range(n_total_layer-1):\n",
        "    z.append(np.matmul(W[i], a[i]) + b[i])\n",
        "\n",
        "    if i < n_total_layer-1:\n",
        "      a.append(sigma(z[i]))\n",
        "\n",
        "  y = make_binary(a[-1])\n",
        "  return y\n",
        "\n",
        "def accuracy_2(x_test, y_test):\n",
        "  # Calculating binary accuracy of the model for 2nd versio\n",
        "  y_predict = []\n",
        "  for row in x_test:\n",
        "    y_predict.append(predict_2(row))\n",
        "\n",
        "  y_predict = np.array(y_predict)\n",
        "  matching_elements = (y_predict == y_test)\n",
        "  count_matches = np.sum(matching_elements)\n",
        "  accuracy = count_matches/x_test.shape[0]\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "bvHiN_DBMzDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load breast cancer classfication Data**"
      ],
      "metadata": {
        "id": "tXzfJKuCuBvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_total, y_total = load_breast_cancer(return_X_y=True)\n",
        "scaler = MinMaxScaler()\n",
        "x_total = scaler.fit_transform(x_total)\n",
        "x, x_test, y_true, y_test = train_test_split(x_total, y_total, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "pj6w0T3FQBeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Algorithm -first version**\n"
      ],
      "metadata": {
        "id": "q--Hsvo_T-1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define neural network\n",
        "n_hidden_layer = 2\n",
        "n_input_layer = x.shape[1]\n",
        "n_layer_2 = 8\n",
        "n_layer_3 = 4\n",
        "n_output_layer = 1\n",
        "\n",
        "# initiate weights\n",
        "W1_shape = (n_layer_2, n_input_layer)\n",
        "W1 = np.random.uniform(-1, 1, W1_shape)\n",
        "b1_shape = (n_layer_2, 1)\n",
        "b1 = np.random.uniform(-1, 1, b1_shape)\n",
        "W2_shape = (n_layer_3, n_layer_2)\n",
        "W2 = np.random.uniform(-1, 1, W2_shape)\n",
        "b2_shape = (n_layer_3, 1)\n",
        "b2 = np.random.rand(*b2_shape)\n",
        "W3_shape = (n_output_layer, n_layer_3)\n",
        "W3 = np.random.uniform(-1, 1, W3_shape)\n",
        "b3_shape = (n_output_layer, 1)\n",
        "b3 = np.random.rand(*b3_shape)\n",
        "\n",
        "alpha = 0.1 # learning rate\n",
        "n_iteration = 100\n",
        "epsilon = 1e-2\n",
        "m = x.shape[0] # number of training examples\n",
        "avg_loss_list = []\n",
        "iteration_num_list = []\n",
        "current_average_loss = 69\n",
        "\n",
        "for iteration in range(n_iteration):\n",
        "  total_loss = 0\n",
        "  for row in range(m):\n",
        "\n",
        "    # forward propagation\n",
        "    a1 = x[row, :].reshape(-1, 1)\n",
        "    z2 = np.matmul(W1, a1) + b1\n",
        "    a2 = sigma(z2)\n",
        "    z3 = np.matmul(W2, a2) + b2\n",
        "    a3 = sigma(z3)\n",
        "    z4 = np.matmul(W3, a3) + b3\n",
        "    y = sigma(z4) # y = a4\n",
        "    loss = binary_entropy_loss(y_true[row], y).item()\n",
        "    total_loss = total_loss + loss\n",
        "\n",
        "    # back propagation\n",
        "    Dz4 = y - y_true[row]\n",
        "    Db3 = Dz4\n",
        "    DW3 = np.matmul(Dz4, a3.T)\n",
        "    Da3 = np.matmul(W3.T, Dz4)\n",
        "    Dz3 = Da3*a3*(1-a3) # element wise multiplication\n",
        "    Db2 = Dz3\n",
        "    DW2 = np.matmul(Dz3, a2.T)\n",
        "    Da2 = np.matmul(W2.T, Dz3)\n",
        "    Dz2 = Da2*a2*(1-a2) # element wise multiplication\n",
        "    Db1 = Dz2\n",
        "    DW1 = np.matmul(Dz2, a1.T)\n",
        "\n",
        "    # update\n",
        "    W1 = W1 - alpha*DW1\n",
        "    b1 = b1 - alpha*Db1\n",
        "    W2 = W2 - alpha*DW2\n",
        "    b2 = b2 - alpha*Db2\n",
        "    W3 = W3 - alpha*DW3\n",
        "    b3 = b3 - alpha*Db3\n",
        "\n",
        "  avg_loss = total_loss/m\n",
        "  avg_loss_list.append(avg_loss)\n",
        "  iteration_num_list.append(iteration)\n",
        "  if abs(current_average_loss - avg_loss)/avg_loss < epsilon:\n",
        "    break\n",
        "  else:\n",
        "    current_average_loss = avg_loss\n",
        "\n",
        "# plot avg loss vs iteration\n",
        "plt.scatter(iteration_num_list, avg_loss_list)\n",
        "\n",
        "# test accuracy\n",
        "accuracy(x_test, y_test)"
      ],
      "metadata": {
        "id": "efXA96mWVQC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Algorithm-second version**"
      ],
      "metadata": {
        "id": "ocxBWqtMt7RZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# user define\n",
        "# neural network architecture\n",
        "n_hidden_layer = 2\n",
        "n_input_layer = x.shape[1]\n",
        "n_layer_2 = 6\n",
        "n_layer_3 = 4\n",
        "# n_layer_4 = 2 # Example of how to add more layers\n",
        "n_output_layer = 1\n",
        "n_total_layer = n_hidden_layer + 2\n",
        "\n",
        "# update the list below with appropriate numbers of hidden layers as defined above\n",
        "n_neuron = [n_input_layer, n_layer_2, n_layer_3, n_output_layer]\n",
        "\n",
        "# initiate weights\n",
        "W = []\n",
        "b = []\n",
        "for i in range(n_total_layer-1):\n",
        "  W.append(np.random.uniform(-1, 1, (n_neuron[i+1], n_neuron[i])))\n",
        "  b.append(np.random.uniform(-1, 1, (n_neuron[i+1], 1)))\n",
        "\n",
        "\n",
        "alpha = 0.1 # learning rate\n",
        "n_iteration = 100\n",
        "epsilon = 1e-2\n",
        "m = x.shape[0] # number of training examples\n",
        "avg_loss_list = []\n",
        "iteration_num_list = []\n",
        "current_average_loss = 99\n",
        "\n",
        "for iteration in range(n_iteration):\n",
        "  total_loss = 0\n",
        "  for row in range(m):\n",
        "    # forward propagation\n",
        "    a = []\n",
        "    z = []\n",
        "    a.append(x[row, :].reshape(-1, 1))\n",
        "    for i in range(n_total_layer-1):\n",
        "      z.append(np.matmul(W[i], a[i]) + b[i])\n",
        "\n",
        "      if i < n_total_layer-1:\n",
        "        a.append(sigma(z[i]))\n",
        "\n",
        "    loss = binary_entropy_loss(y_true[row], a[i+1]).item()\n",
        "    total_loss = total_loss + loss\n",
        "\n",
        "    # back propagation\n",
        "    Dz = []\n",
        "    DW = []\n",
        "    Db = []\n",
        "    Da = []\n",
        "    Dz.insert(0, a[-1] - y_true[row])\n",
        "    for i in range(n_total_layer-2, -1, -1):\n",
        "      Db.insert(0, Dz[0])\n",
        "      DW.insert(0, np.matmul(Dz[0], a[i].T))\n",
        "      Da.insert(0, np.matmul(W[i].T, Dz[0]))\n",
        "      if i>0:\n",
        "        Dz.insert(0, Da[0]*a[i]*(1-a[i]))\n",
        "\n",
        "    # update\n",
        "    for i in range(n_total_layer-1):\n",
        "      W[i] = W[i]-alpha*DW[i]\n",
        "      b[i] = b[i]-alpha*Db[i]\n",
        "\n",
        "  # storing avg loss and applying stopping condition\n",
        "  avg_loss = total_loss/m\n",
        "  avg_loss_list.append(avg_loss)\n",
        "  iteration_num_list.append(iteration)\n",
        "  if abs(current_average_loss - avg_loss)/avg_loss < epsilon:\n",
        "    break\n",
        "  else:\n",
        "    current_average_loss = avg_loss\n",
        "\n",
        "# plotting loss vs iteration step\n",
        "plt.scatter(iteration_num_list, avg_loss_list)\n",
        "\n",
        "# calculating accuracy\n",
        "print(accuracy_2(x_test, y_test))"
      ],
      "metadata": {
        "id": "KH-VSm_7O1Cc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
